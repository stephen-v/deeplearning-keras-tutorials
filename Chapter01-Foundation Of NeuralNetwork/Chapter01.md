
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1. 神经网络基础](#1-神经网络基础)
	* [1.1. 张量(Tensor)——神经网络的数据表示](#11-张量tensor神经网络的数据表示)
		* [1.1.1. 1D张量](#111-1d张量)
		* [1.1.2. 2D张量](#112-2d张量)
		* [1.1.3. 3D张量](#113-3d张量)
		* [1.1.4. 4D张量](#114-4d张量)
	* [1.2. 张量基本运算](#12-张量基本运算)
		* [1.2.1. Hadamard 乘积](#121-hadamard-乘积)
		* [1.2.2. 点积](#122-点积)
	* [1.3. 基于梯度的优化方法](#13-基于梯度的优化方法)
		* [1.3.1. 随机梯度下降](#131-随机梯度下降)
	* [1.4. 神经网络的前向传播](#14-神经网络的前向传播)
	* [1.5. 神经网络的反向传播](#15-神经网络的反向传播)

<!-- /code_chunk_output -->



# 1. 神经网络基础

## 1.1. 张量(Tensor)——神经网络的数据表示

>一般来说，当前所有机器学习系统都使用张量作为基本数据结构。张量对这个领域非常重要，重要到Google的TensorFlow都以它来命名。那么什么是张量？张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它是数字的容器。你可能对矩阵很熟悉，它是二维张量。张量是矩阵向任意维度的推广［注意，张量的维度（dimension）通常叫作轴（axis）］ 

### 1.1.1. 1D张量
仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D 张量）。在 Numpy
中，一个 float32 或 float64 的数字就是一个标量张量（或标量数组）。你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴（ ndim == 0 ）。张量轴的个数也叫作阶 **（rank）**。下面是一个 Numpy 标量。

```Python
import numpy as np 
x = np.array(12)
ndim =x.ndim
```

![2019-02-19-14-00-59](http://www.xdpie.com/2019-02-19-14-00-59.png)

### 1.1.2. 2D张量
向量组成的数组叫作矩阵（matrix）或二维张量（2D 张量）。矩阵有 2 个轴（通常叫作行和列）。你可以将矩阵直观地理解为数字组成的矩形网格。
```Python
x = np.array([[5, 78, 2, 34, 0],
            [6, 79, 3, 35, 1],
            [7, 80, 4, 36, 2]])
ndim =x.ndim
```
![2019-02-19-14-40-40](http://www.xdpie.com/2019-02-19-14-40-40.png)
列举两个二维张量
* 人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。
* 文本文档数据集，我们将每个文档表示为每个单词在其中出现的次数（字典中包含20000个常见单词）。每个文档可以被编码为包含 20 000 个值的向量（每个值对应于字典中每个单词的出现次数），整个数据集包含 500 个文档，因此可以存储在形状为(500, 20000) 的张量中

### 1.1.3. 3D张量
将多个矩阵组合成一个新的数组，可以得到一个 3D 张量，你可以将其直观地理解为数字组成的立方体。下面是一个 Numpy 的 3D 张量。
```Python
x = np.array([[[5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]],
                [[5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]],
                [[5, 78, 2, 34, 0],
                [6, 79, 3, 35, 1],
                [7, 80, 4, 36, 2]]])
ndim =x.ndim
```
![2019-02-19-14-49-37](http://www.xdpie.com/2019-02-19-14-49-37.png)
当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的 3D 张量中。每个样本可以被编码为一个向量序列（即 2D 张量），因此一个数据批量就被编码为一个 3D 张量.
![2019-02-19-14-53-38](http://www.xdpie.com/2019-02-19-14-53-38.png)

### 1.1.4. 4D张量

图像通常具有三个维度：高度、宽度和颜色深度。虽然灰度图像（比如 MNIST 数字图像）只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则,可以保存在一个形状为 (128, 256, 256, 3) 的4D张量中
![2019-02-19-14-52-15](http://www.xdpie.com/2019-02-19-14-52-15.png)

## 1.2. 张量基本运算

### 1.2.1. Hadamard 乘积
特别地，假设 a 和 b 是两个同样维度的张量，那么我们使⽤ s ⊙ t 来表⽰按元素的乘积或者hadamrad乘积。
![2019-02-19-15-41-53](http://www.xdpie.com/2019-02-19-15-41-53.png)

```Python
a = np.array([1,1,1])
b = np.array([2,2,2])
z=  np.multiply(a,b)
```

![2019-02-19-15-44-16](http://www.xdpie.com/2019-02-19-15-44-16.png)

### 1.2.2. 点积
点积运算，也叫张量积（tensor product，不要与逐元素的乘积弄混），是最常见也最有用张量运算。与逐元素的运算不同，它将输入张量的元素合并在一起。在 Numpy、Keras、Theano 和 TensorFlow 中，都是用 * 实现逐元素乘积。TensorFlow 中的点积使用了不同的语法，但在 Numpy 和 Keras 中，都是用标准的 dot 运算符来实现点积。
![2019-02-19-15-17-13](http://www.xdpie.com/2019-02-19-15-17-13.png)
![2019-02-19-15-17-29](http://www.xdpie.com/2019-02-19-15-17-29.png)
则a和b的点积公式为：
![2019-02-19-15-17-47](http://www.xdpie.com/2019-02-19-15-17-47.png)
>这里要求张量a和张量b的行列数相同

```Python
a = np.array([1,1,1])
b = np.array([2,2,2])
z= np.dot(a,b)
```
![2019-02-19-15-20-25](http://www.xdpie.com/2019-02-19-15-20-25.png)

两个相同维数的张量 a 和 b 的点积（dot product）可看作是矩阵乘积 $a^T$ b

```Python
z= np.matmul(a.T,b)
```

![2019-02-19-15-50-04](http://www.xdpie.com/2019-02-19-15-50-04.png)



## 1.3. 基于梯度的优化方法
用一对$(x,y)$来表示一个单独的样本，$x$代表$n$维的特征向量，$y$表示标签(输出结果）只能为0或1。 而训练集将由𝑛个训练样本组成，其中$(x(1) ,y(1))$表示第一个样本的输入和输出，$(x(2) ,y(2))$表示第二个,以此类推。我们用函数 
$$
y(x)=wx+b \tag*{1.1}
$$ 
来近似表示$y$与$x$的线性关系。那么显然有如下函数表示预测值$\hat y$ 与真实值 $y$的偏差，我们把C称为损失函数 **(也称为代价或⽬标函数)** ；有时也称被称为均⽅误差或者 MSE：
$$
C(w,b)=\frac{1}{2n}\ \sum\limits_{x}y(x)-\hat y(x) \tag*{1.2}
$$

观察⼆次代价函数的形式我们可以看到 $C(w,b)$ 是⾮负的，因为求和公式中的每⼀项都是⾮负的。此外，代价函数 $C(w,b)$的值相当⼩，即 $C(w,b) ≈ 0$，精确地说，是当对于所有的训练输⼊ x，y(x) 接近于输出 $\hat y(x)$ 时。因此如果我们的学习算法能找到合适的权重和偏置，使得 $C(w,b) ≈ 0$，它就能很好地⼯作。相反，当 $C(w,b)$ 很⼤时就不怎么好了，那以为着对于⼤量地输⼊，$y(x)$ 与输出 $\hat y(x)$ 相差很⼤。因此我们的训练算法的⽬的，是最⼩化权重和偏置的代价函数 $C(w,b)$。换句话说，我们想要找到⼀系列能让代价尽可能⼩的权重和偏置。我们将采⽤称为梯度下降的算法来达到这个⽬的。

好了，假设我们要最⼩化函数，$C(v)$。它可以是任意的多元实值函数，$v = v 1 ,v 2$ ,...。注意我们⽤ $v$ 代替了式(1.1)中 $w$ 和 $b$ 以强调它可能是任意的函数 —— 我们现在先不局限于神经⽹络的环境。为了最⼩化 $C(v)$，想象 $C(v)$ 是⼀个只有两个变量 $v1$ 和 $v2$ 的函数.
![2019-02-19-16-19-03](http://www.xdpie.com/2019-02-19-16-19-03.png)

我们想要的是找到 $C(v)$ 的全局最⼩值。当然，对于上图的函数，我们⼀眼就能找到最⼩值。那只意味着，也许我展⽰的函数过于简单了！通常函数 $C(v)$可能是⼀个复杂的多元函数，看⼀下就能找到最⼩值可是不可能的。
⼀种解决这个问题的⽅式是⽤微积分来解析最⼩值。我们可以计算导数去寻找 C 的极值点。
运⽓好的话，C 是⼀个只有⼀个或少数⼏个变量的函数。但是变量过多的话那就是噩梦。⽽且
神经⽹络中我们经常需要⼤量的变量——最⼤的神经⽹络有依赖数亿权重和偏置的代价函数，极
其复杂。⽤微积分来计算最⼩值已经不可⾏了。

好吧，微积分是不能⽤了。幸运的是，有⼀个漂亮的推导法暗⽰有⼀种算法能得到很好的效
果。⾸先把我们的函数想象成⼀个⼭⾕。只要瞄⼀眼上⾯的绘图就不难理解。我们想象有⼀个
⼩球从⼭⾕的斜坡滚落下来。当小球分别从v1,v2两个方向移动很小的距离，C(v)将发生如下变化：
$$
\Delta{C(v)} = \frac{\partial{C}}{\partial{V_1}}\Delta{v_1}  +  \frac{\partial{C}}{\partial{V_2}}\Delta{v_2} \tag*{1.3}
$$ 

我们要寻找⼀种选择 $∆v_1$ 和 $∆v_2$ 的⽅法使得 $∆C$ 为负；即，我们选择它们是为了让球体滚落。为了弄明⽩如何选择，需要定义 $∆v$ 为 $v$ 变化的向量，$∆v ≡ (∆v_1 ,∆v_2 )^T$

我们也定义 $C$ 的梯度为偏导数的向量，我们⽤ $∇C$ 来表⽰梯度向量，即
$$
\nabla{C} = (\frac{\partial{C}}{\partial{V_1}},\frac{\partial{C}}{\partial{V_2}}) \tag*{1.4}
$$
则我们可以式(1.3)改写为如下形式：
$$
\Delta{C(v)} = \nabla{C} \cdot \Delta(v) \tag*{1.5}
$$ 
现在假定 $\Delta(v)=-\eta \cdot \nabla{C}$,将此式带入式（1.5）中得到
$$
\Delta{C(v)} = -\eta ||\nabla{C}||^2 \tag*{1.6}
$$ 
由于 $∥∇C∥^2 ≥ 0$，这保证了 $∆C ≤ 0$,这正是我们想要的特性,因此用$\Delta(v)=-\eta \cdot \nabla{C}$来描述物体梯度下降的规律，用来计算$∆v$，来移动球体的位置 $v$。
$$
v → v ′ = v − η∇C \tag*{1.7}
$$
然后我们⽤它再次更新规则来计算下⼀次移动。如果我们反复持续这样做，我们将持续减⼩
C 直到获得⼀个全局的最⼩值。总结⼀下，梯度下降算法⼯作的⽅式就是重复计算梯度 ∇C，然后沿着相反的⽅向移动，沿着⼭⾕“滚落”。我们可以想象它像这样.
![2019-02-19-16-48-40](http://www.xdpie.com/2019-02-19-16-48-40.png)

### 1.3.1. 随机梯度下降
应⽤梯度下降规则有很多挑战，为了计算$\Delta{C}$需要花费很长时间，在实践中，为了计算梯度 $∇C$，我们需要为每个训练输⼊ $x$ 单独地计算梯度值 $∇C(x)$ ，然后求平均值$\nabla{C} =\frac{1}{n}\sum\limits_{x}{C_x}$。不幸的是，当训练输⼊的数量过⼤时会花费很⻓时间，这样会使学习变得相当缓慢。
有种叫做随机梯度下降的算法能够加速学习。其思想就是通过随机选取⼩量训练输⼊样本来计算 $∇C(x)$ ，进⽽估算梯度 $∇C$。通过计算少量样本的平均值我们可以快速得到⼀个对于实际梯度 $∇C$ 的很好的估算，这有助于加速梯度下降，进⽽加速学习过程。

更准确地说，随机梯度下降通过随机选取⼩量的 m 个训练输⼊来⼯作,假设样本数量m ⾜够⼤，即
$$
\nabla{C} = \frac{1}{n}\sum\limits_{x=1}\limits^{n}{C_x} ≈ \frac{1}{m}\sum\limits_{x=1}\limits^{m}{C_x} \tag*{1.8}
$$

## 1.4. 神经网络的前向传播
下图显示了具有3层、每层具有3个节点的神经网络示例。为了保证图的清晰，我们并没有标上所有的权重，此处，我们要向大家介绍一些常用的术语。正如我们所知道的，第一层为输入层，最后一层为输出层，中间层我们称之为隐藏层。虽然隐藏层听起来很神秘、很黑暗，但是很遗憾，我们这样称呼中间层，其理由并不神秘。中间层的输出不需要很明显地表示为输出，因此我们称之为“隐藏”层。当然，这个解释有点牵强，然而，我们对这个名称没有更好的解释了。

![2019-02-20-10-07-47](http://www.xdpie.com/2019-02-20-10-07-47.png)

让我们演示一下图中所描述的示例网络。我们观察到了3个输入是0.9、0.1和0.8。因此，输入矩阵$I$为。
$$
I =\begin{bmatrix}
0.9  \\
0.1  \\
0.8
\end{bmatrix}
$$
接下来是中间的隐藏层。在这里，我们需要计算出输入到中间层每个节点的组合（调节）信号。请记住，中间隐藏层的每个节点都与输入层的每个节点相连，因此每个节点都得到输入信号的部分信息。输入到中间层的组合调节信号为$X = W_{ih}•I$，其中$I$为输入信号矩阵，$W$为权重矩阵。这个神经网络的$I$、$W$是什么样的呢？图中显示了这个神经网络的一些权重，但是并没有显示所有的权重。下图显示了所有的权重，同样，这些数字是随机列举的。在这个示例中，权重没有什么特殊的含义
$$
W_{ih} =\begin{bmatrix}
0.9   &   0.3 &  0.4\\
0.2   &   0.8 &  0.2 \\
0.1   &   0.5 &  0.6
\end{bmatrix}
$$

你可以看到，第一个输入节点和中间隐藏层第一个节点之间的权重为w 1,1 = 0.9，正如上图中的神经网络所示。同样，你可以看到输入的第二节点和隐藏层的第二节点之间的链接的权重为w 2,2 = 0.8。图中并没有显示输入层的第三个节点和隐藏层的第一个节点之间的链接，我们随机编了一个权重w 3,1 = 0.4。

是等等，为什么这个$W$的下标我们写的是“ih”呢？这是因为$W_{ih}$是输入层和隐藏层之间的权重。我们需要另一个权重矩阵来表示隐藏层和输出层之间的链接，这个矩阵我们称之为$W_{ho}$。
$$
W_{ho} =\begin{bmatrix}
0.9   &   0.3 &  0.4\\
0.2   &   0.8 &  0.2 \\
0.1   &   0.5 &  0.6
\end{bmatrix}
$$
我们一起继续算出输入到隐藏层的组合调节输入值。我们应该给这个输入值一个名称，考虑到这个组合输入是到中间层，而不是最终层，因此，我们将它称为$X_h$
$$
X_h =   W_{ih} \cdot I
$$


```Python
I = np.array([0.9,0.1,0.8]).T
W_ih=np.array([[0.9,0.3,0.4],[0.2,0.8,0.2],[0.1 ,0.5 ,0.6]])
X = np.matmul(W_ih,I)
```

![2019-02-20-10-41-04](http://www.xdpie.com/2019-02-20-10-41-04.png)


$$
X_h =  \begin{bmatrix}
1.16  \\
0.42 \\
0.62
\end{bmatrix}
$$

我们已经拥有了输入到中间隐藏层的组合调节输入值，它们为1.16、0.42和0.62。我们使用矩阵这个工具来完成这种复杂的工作，这是值得我们自豪的一个成就。

让我们可视化这些输入到第二层隐藏层的组合调节输入。

![2019-02-20-10-44-40](http://www.xdpie.com/2019-02-20-10-44-40.png)

虽然到目前为止，一切都还顺利，但是，我们还有更多的工作要做。我们对这些节点应用了S激活函数，使得节点对信号的反应更像自然界中节点的反应，因此，现在，我们进行这个操作：
>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）

Sigmoid 是常用的非线性的激活函数，它的数学形式如下：
$$
f(z) = \frac{1}{1+e^{-z}} \tag*{1.9}
$$
![2019-02-20-10-50-45](http://www.xdpie.com/2019-02-20-10-50-45.png)

则最终的隐藏层输出如下：
$$
O_h= sigmoid(X_h)
$$
以此类推从隐藏层到输出层的过程也如上，此处不再赘述。

## 1.5. 神经网络的反向传播
在式(1.2)提到损失函数$C$，用于评估神经网络的输出与实际结果的误差，对神经网络训练的目的是为了使损失函数最小，在训练的过程中采用了梯度下降的方法，通过梯度下降来更新权重W，在神经网络中由于有多层每层有多个节点，每层都有自己的权重，因此如果将更新至分配到不同层权重中去是一个比较复杂的问题。
![2019-02-20-11-10-47](http://www.xdpie.com/2019-02-20-11-10-47.png)
使用所有的误差值，只对一个权重进行更新，这种做法忽略了其他链接及其权重，毫无意义。多条链接都对这个误差值有影响。

只有一条链接造成了误差，这种机会微乎其微。如果我们改变了已经“正确”的权重而使情况变得更糟，那么在下一次迭代中，这个权重就会得到改进，因此神经网络并没有失去什么。

一种思想就是在所有造成误差的节点中平分误差，如下图所示。

![2019-02-20-11-11-45](http://www.xdpie.com/2019-02-20-11-11-45.png)

另一种思想是不等分误差。与前一种思想相反，我们为较大链接权重的全待接分配更多的误差。为什么这样做呢？这是因为这些链接对造成误差的贡献较大。下图详细阐释了这种思想。

![2019-02-20-11-12-03](http://www.xdpie.com/2019-02-20-11-12-03.png)

此处，有两个节点对输出节点的信号做出了贡献，它们的链接权重分别是3.0和1.0。如果按权重比例分割误差，那么我们就可以观察到输出误差的3/4应该可以用于更新第一个较大的权重，误差的1/4 可以用来更新较小的权重。

我们可以将同样的思想扩展到多个节点。如果我们拥有100个节点链接到输出节点，那么我们要在这100条链接之间，按照每条链接对误差所做贡献的比例（由链接权重的大小表示），分割误差。

你可以观察到，我们在两件事情上使用了权重。第一件事情，在神经网络中，我们使用权重，将信号从输入向前传播到输出层。此前，我们就是在大量地做这个工作。第二件事情，我们使用权重，将误差从输出向后传播到网络中。我们称这种方法为反向传播，你应该不会对此感到惊讶吧。

